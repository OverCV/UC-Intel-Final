\chapter{Experimentos y Resultados}
\label{chap:experiments}

Este capítulo presenta los resultados experimentales obtenidos al verificar las tres hipótesis planteadas en el Capítulo \ref{chap:introduction}. Cada sección corresponde a un experimento diseñado específicamente para evaluar una hipótesis, incluyendo configuración, resultados y verificación formal.

% =============================================================================
\section{Configuración Experimental General}
\label{sec:exp:setup}

\subsection{Entorno de Ejecución}

Todos los experimentos se ejecutaron bajo las siguientes condiciones:
\begin{itemize}
    \item \textbf{Hardware:} NVIDIA GPU (especificar modelo), XX GB VRAM
    \item \textbf{Framework:} PyTorch 2.x con CUDA 12.x
    \item \textbf{Reproducibilidad:} Semilla aleatoria fija (seed=42) para todas las ejecuciones
    \item \textbf{Early Stopping:} Paciencia de 10 épocas sobre validation loss
\end{itemize}

\subsection{Dataset: MalImg}

El dataset MalImg contiene imágenes en escala de grises derivadas de ejecutables de malware de Windows. La Tabla \ref{tab:malimg-distribution} muestra la distribución de muestras por familia.

\begin{table}[h]
\centering
\caption{Distribución de familias en el dataset MalImg}
\label{tab:malimg-distribution}
\begin{tabular}{lcc}
\hline
\textbf{Familia} & \textbf{Muestras} & \textbf{Proporción (\%)} \\
\hline
Alueron.gen!J       & 198   & 2.1  \\
C2LOP.P             & 146   & 1.6  \\
C2LOP.gen!g         & 200   & 2.2  \\
Dontovo.A           & 162   & 1.8  \\
Fakerean            & 381   & 4.1  \\
Instantaccess       & 431   & 4.7  \\
Lolyda.AA 1         & 213   & 2.3  \\
Lolyda.AA 2         & 184   & 2.0  \\
Lolyda.AA 3         & 123   & 1.3  \\
Lolyda.AT           & 159   & 1.7  \\
Malex.gen!J         & 136   & 1.5  \\
Obfuscator.AD       & 142   & 1.5  \\
... (continúa)      & ...   & ...  \\
\hline
\textbf{Total}      & \textbf{9,339} & \textbf{100.0} \\
\hline
\end{tabular}
\end{table}

\textbf{Observación sobre desbalance:} Se identifican 5 familias con menos de 100 muestras (clases minoritarias), lo cual motiva la evaluación de data augmentation en H2.

\subsection{Partición de Datos}

\begin{itemize}
    \item \textbf{Entrenamiento:} 70\% (6,537 muestras)
    \item \textbf{Validación:} 15\% (1,401 muestras)
    \item \textbf{Prueba:} 15\% (1,401 muestras)
    \item \textbf{Estratificación:} Sí, manteniendo proporciones por clase
\end{itemize}

% =============================================================================
\section{Experimento 1: Comparación de Arquitecturas (H1)}
\label{sec:exp:h1}

\subsection{Objetivo}

Verificar la hipótesis H1: \textit{``ResNet50 pre-entrenado en ImageNet con fine-tuning superará tanto a una CNN custom como a un Vision Transformer en accuracy y F1-score macro.''}

\subsection{Configuración de Modelos}

Se evaluaron cuatro arquitecturas: un baseline convencional, una CNN profunda de 5 bloques, ResNet50 con fine-tuning, y Vision Transformer.

\subsubsection{Conventional CNN (Baseline)}
\begin{itemize}
    \item \textbf{Arquitectura:} 2 bloques convolucionales (configuración estándar del curso)
    \item \textbf{Estructura:} Conv2D + MaxPool + Conv2D + MaxPool + Flatten + Dense
    \item \textbf{Propósito:} Establecer línea base de comparación
\end{itemize}

\subsubsection{VGG-Mini-H1 (5 bloques)}
\begin{itemize}
    \item \textbf{Arquitectura:} 5 bloques convolucionales (32$\rightarrow$64$\rightarrow$128$\rightarrow$256$\rightarrow$512 filtros)
    \item \textbf{Cada bloque:} Conv2D + BatchNorm + ReLU + MaxPool(2$\times$2)
    \item \textbf{Clasificador:} GlobalAvgPool + Dropout(0.5) + Dense(256) + Dropout(0.3) + Output
\end{itemize}

\subsubsection{ResNet50 (Fine-tuning)}
\begin{itemize}
    \item \textbf{Backbone:} ResNet50 pre-entrenado en ImageNet
    \item \textbf{Estrategia:} Partial fine-tuning (20-30 últimas capas descongeladas)
    \item \textbf{Clasificador:} Dropout(0.5) + Dense(512) + Output
\end{itemize}

\subsubsection{Vision Transformer (ViT-Small)}
\begin{itemize}
    \item \textbf{Patch size:} 16$\times$16
    \item \textbf{Embedding dimension:} 384
    \item \textbf{Depth:} 12 transformer blocks
    \item \textbf{Attention heads:} 6
    \item \textbf{MLP ratio:} 4.0
    \item \textbf{Dropout:} 0.1
\end{itemize}

% --- Archivos modulares ---
\input{Chapters/Experiments/h1-tables}

\input{Chapters/Experiments/h1-figures}

\input{Chapters/Experiments/h1-analysis}

\input{Chapters/Experiments/h1-verification}

% =============================================================================
\section{Experimento 2: Impacto de Data Augmentation (H2)}
\label{sec:exp:h2}

\subsection{Objetivo}

Verificar la hipótesis H2: \textit{``Data augmentation moderada mejorará significativamente el recall de las familias de malware minoritarias, sin degradar sustancialmente el accuracy global del modelo.''}

\subsection{Configuración}

Se utilizó el mejor modelo de H1 como arquitectura base, entrenando dos versiones: una sin augmentation y otra con augmentation moderada.

% --- Archivos modulares ---
\input{Chapters/Experiments/h2-tables}

\input{Chapters/Experiments/h2-figures}

\input{Chapters/Experiments/h2-analysis}

\input{Chapters/Experiments/h2-verification}



% =============================================================================
\section{Experimento 3: Efecto de la Profundidad en CNN (H3)}
\label{sec:exp:h3}

\subsection{Objetivo}

Verificar la hipótesis H3: \textit{``Incrementar la profundidad de una CNN custom mejorará el rendimiento del modelo, pero con rendimientos decrecientes y mayor costo computacional.''}

\subsection{Configuración de Arquitecturas}

Se compararon dos arquitecturas CNN con diferente profundidad, manteniendo la misma estructura de bloque (Conv + BN + ReLU + MaxPool) y clasificador.


Para abordar esta hipótesis, se diseñó un enfoque metodológico sistemático que involucra:
\begin{enumerate}
    \item Diseño de dos arquitecturas CNN con profundidad incremental pero manteniendo coherencia estructural
    \item Entrenamiento bajo condiciones controladas (mismo optimizador, épocas, conjunto de datos)
    \item Evaluación exhaustiva mediante métricas múltiples (accuracy, loss, F1, brecha de generalización)
    \item Análisis comparativo de curvas de aprendizaje y estabilidad del entrenamiento
    \item Medición indirecta del costo computacional mediante inferencia estructural
\end{enumerate}

La lógica experimental se basa en el principio de \textit{ceteris paribus} (todo lo demás constante), donde solo la profundidad de la red varía entre condiciones, permitiendo aislar su efecto específico sobre el rendimiento y eficiencia.



\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth,height=5cm]{Figures/H3/c1.jpg}
\includegraphics[width=0.45\textwidth,height=5cm]{Figures/H3/c2.jpg}
\caption{Configuración para los diferentes modelos}
\label{fig:curves-exp3}
\end{figure}


\subsection{Configuración Entrenamiento}

\subsubsection{Especificaciones Técnicas Detalladas}

Las arquitecturas comparadas comparten una filosofía de diseño modular basada en bloques convolucionales secuenciales, pero difieren en el número de estos bloques. La Tabla \ref{tab:h3-architectures-detailed} presenta una descomposición técnica exhaustiva.

\begin{table}[h]
\centering
\caption{Análisis técnico detallado de las arquitecturas CNN comparadas}
\label{tab:h3-architectures-detailed}
\begin{tabularx}{\textwidth}{|l|X|X|}
\hline
\textbf{Aspecto} & \textbf{H2\_MOD.A (9 capas)} & \textbf{Arquitectura de 12 Capas} \\
\hline
\textbf{Filosofía de Diseño} & 
Arquitectura moderadamente profunda con 2 bloques convolucionales. Enfoque en extracción jerárquica de características sin complejidad excesiva. &
Extensión profunda con 3 bloques convolucionales para mayor capacidad de representación. 
\\ \hline

\textbf{Flujo de Datos} &
Entrada $\rightarrow$ Bloque1(Conv+Conv+Pool) $\rightarrow$ Bloque2(Conv+Conv+Pool) $\rightarrow$ Regularización(Dropout) $\rightarrow$ Clasificador(Flatten+Dense) $\rightarrow$ Salida &
Entrada $\rightarrow$ Bloque1(Conv+Conv+Pool) $\rightarrow$ Bloque2(Conv+Conv+Pool) $\rightarrow$ Bloque3(Conv+Conv+Pool) $\rightarrow$ Regularización(Dropout) $\rightarrow$ Clasificador(Flatten+Dense) $\rightarrow$ Salida
\\ \hline

\textbf{Capacidad de Representación} & 210,000 parámetros entrenables aprox. & 280,000 parámetros entrenables aprox. (+33\%). 
\\ \hline

\textbf{Regularización} & Dropout único del 25\%. & Dropout del 25\% aplicado al tercer bloque. 
\\ \hline

\textbf{Hiperparámetros Comunes} &
Optimizador Adam (lr=0.001), 10 épocas, Batch Size 32, activación ReLU, función de pérdida Categorical Crossentropy. &
Idéntica configuración.
\\ \hline

\end{tabularx}
\end{table}


\subsubsection{Consideraciones de Implementación}

Ambos modelos fueron implementados en TensorFlow 2.x con inicialización He normal. El preprocesamiento incluyó normalización de píxeles (0-1) sin aumentación para aislar el efecto de la profundidad. Entrenamiento en GPU Nvidia T4 (16GB), verificando uso de memoria para evitar limitaciones.

\subsection{Resultados Cuantitativos y Comparación Exhaustiva}

\subsubsection{Métricas de Rendimiento Principal}

\begin{table}[h]
\centering
\caption{Análisis cuantitativo detallado del rendimiento por arquitectura}
\label{tab:h3-results-detailed}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{3.5cm}|c|c|c|p{6cm}|}
\hline
\textbf{Categoría} & \textbf{Métrica} & \textbf{H2\_MOD.A (9 capas)} & \textbf{12 Capas} & \textbf{Interpretación} \\ \hline

\multirow{2}{*}{Precisión} 
& Val Accuracy (\%) & \textbf{85.29} & 83.45 & Disminución con mayor profundidad \\ \cline{2-5}
& Test Accuracy (\%) & N/D & \textbf{85.58} & Generaliza moderadamente bien \\ \hline

\multirow{2}{*}{Pérdida}
& Val Loss & \textbf{0.3677} & 0.4095 & Mayor error residual en modelo profundo \\ \cline{2-5}
& Train Loss & 0.2644 & 0.3061 & Dificultad de optimización \\ \hline

\multirow{3}{*}{Generalización}
& Brecha (pp) & 2.86 & 4.13 & Aumento de sobreajuste \\ \cline{2-5}
& Ratio Train/Val Loss & 1.39 & 1.33 & Menor estabilidad \\ \cline{2-5}
& Consistencia Épocas Finales & Alta & Media & Volatilidad \\ \hline

\multirow{3}{*}{Métricas por Clase}
& Macro F1 (\%) & N/D & 83.54 & Rendimiento no equilibrado \\ \cline{2-5}
& Weighted F1 (\%) & N/D & 85.54 & Dominado por clases mayoritarias \\ \cline{2-5}
& Desviación Std F1 & N/D & $\sim$8.2 & Variabilidad entre clases \\ \hline

\multirow{2}{*}{Eficiencia}
& Duración Entrenamiento & 33m 48s & $\sim$45m & +33\% estimado \\ \cline{2-5}
& Memoria GPU & $\sim$3.2GB & $\sim$4.1GB & +28\% estimado \\ \hline

\end{tabular}
}
\end{table}


\subsubsection{Curvas de Aprendizaje}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \caption{Curvas de Loss: mayor volatilidad en 12 capas}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \caption{Curvas de Accuracy: convergencia inferior en modelo profundo}
    \end{subfigure}
    \caption{Comparación visual de dinámicas de aprendizaje}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth,height=5cm]{Figures/H3/ima2.jpg}
\includegraphics[width=0.45\textwidth,height=5cm]{Figures/H3/ima2.jpg}
\caption{Comparación de curvaturas de entrenamiento para el Experimento: Accuracy (izquierda) y Loss (derecha).}
\label{fig:curves-exp3}
\end{figure}

\subsubsection{Análisis de la Brecha de Generalización}

La brecha se incrementó 44.4\% (2.86pp a 4.13pp), atribuible a:

\begin{itemize}
    \item Relación parámetros/datos más desfavorable
    \item Desvanecimiento parcial de gradientes sin conexiones residuales
    \item Co-adaptación de características con menor generalización
\end{itemize}

\subsection{Análisis de la Hipótesis por Componente}

\subsubsection{Rendimientos Decrecientes}

\begin{enumerate}
    \item El accuracy disminuyó (85.29\% $\rightarrow$ 83.45\%).
    \item Aumento del 33\% en parámetros no mejoró el rendimiento.
    \item Rentabilidad marginal negativa: -0.0055\% por 1000 parámetros.
\end{enumerate}

\subsubsection{Costo Computacional}

\begin{itemize}
    \item Forward Pass:
    \begin{align*}
        \text{H2\_MOD.A} &: \sim 85 \text{ MFLOPs} \\
        \text{12 Capas} &: \sim 115 \text{ MFLOPs}
    \end{align*}
    \item Memoria:
    \begin{align*}
        45\text{MB} \rightarrow 62\text{MB}
    \end{align*}
    \item Tiempo por época:
    \begin{align*}
        3.38 \text{ min} \rightarrow 4.57 \text{ min}
    \end{align*}
\end{itemize}

\subsection{Verificación Sistemática de H3}

\begin{table}[h]
\centering
\caption{Verificación cuantitativa de componentes de H3}
\label{tab:h3-hypothesis-verification}
\begin{tabular}{|p{6cm}|c|p{7cm}|}
\hline
\textbf{Componente} & \textbf{Verificación} & \textbf{Evidencia} \\ \hline
``Mayor profundidad mejora rendimiento'' & Rechazada & Accuracy menor, pérdida mayor \\
``Rendimientos decrecientes'' & Confirmada & Más recursos no $\rightarrow$ mejor resultado \\
``Mayor costo computacional'' & Confirmada & +35\% FLOPs, +33\% tiempo, +28\% memoria \\
``Mayor capacidad de patrones'' & Parcial & Test Accuracy positivo pero F1 desigual \\
\hline
\end{tabular}
\end{table}





\subsubsection{Conclusión Final}

Este experimento demuestra que incrementar la profundidad sin técnicas complementarias no garantiza mayor rendimiento. La profundidad aporta capacidad, pero también costos y riesgo de saturación temprana. La efectividad se encuentra en un balance entre profundidad, generalización y eficiencia computacional.



